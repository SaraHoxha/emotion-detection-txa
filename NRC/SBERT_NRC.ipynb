{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e1eb97-8fc3-4d63-bd4e-fe110175167e",
   "metadata": {},
   "source": [
    "# Predict the Emotion Labels based on the Basic NRC Emotion Lexicon "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961be50c-1d82-407c-a89d-aed58c6b0ac5",
   "metadata": {},
   "source": [
    "In this section, we implemented the predictation emotion label to the texts in both t5 and yangswei_85 dataset. Firstly, we began by using the SBERT model to transform the dataset's text into embedding vectors. Next, we organized the emotion lexicons from the Basic NRC Emotion Lexicon into groups and converted them into SBERT embeddings as well. Next step, we calculated the cosine similarity between the text embeddings and the emotion embeddings. The emotion with the highest similarity score was assigned as the predicted emotion for the text. Finally, we mapped the emotion that predicted by NRC Emotion Lexicon into Parrott's emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff62451-225e-4a6e-a46b-7686bd7f44ab",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8d9b73-c43a-46af-bf35-12b405b3eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\minhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5887e743-07cc-449d-9edd-ee31d25369ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5389d670-a693-4627-87e4-e763eee1e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643682b9-15de-4877-b057-cffe5ff3b086",
   "metadata": {},
   "source": [
    "## T5 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292944b-a765-46b6-8b67-ad179b11bcd3",
   "metadata": {},
   "source": [
    "From this part, we implemented the code based on the t5_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5a2755-b3cd-40a5-932c-a0e4d757744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_test = pd.read_csv('https://raw.githubusercontent.com/SaraHoxha/emotion-detection-txa/main/Model%20Implementation/data/test_t5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567463e2-eb4d-41e3-a28c-bbc987d3b4dc",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce591192-22db-4751-a18f-f124abf6fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\minhd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\minhd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "download('punkt')  # For tokenization\n",
    "download('stopwords')  # For stopwords\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4fbf05-8a6a-44f0-904f-c47994b764a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase \n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Lemmatization (using SpaCy)\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    # Return to string \n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c62ae9-3832-4ecc-9dc9-f4b9cae5aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "t5_test['processed_text'] = t5_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001707c-0c25-42a2-ac6e-4728823a0697",
   "metadata": {},
   "source": [
    "## Text Embedding with SBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c96b2-5b78-4686-9db1-4d164b41e8af",
   "metadata": {},
   "source": [
    "This part, we used SBERT (Sentence-BERT) to convert text into vector representations, which capture the meaning of each sentence. The model all-mpnet-base-v2 is used to generate these embeddings. Each text in the dataset is transformed into an embedding and stored in a new column called embeddings. These embeddings can later be compared to emotion embeddings to predict the emotions behind the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224e7124-c441-41d8-9e1f-f998bf1e10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SBERT to generate sentence embeddings\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d76831-64c4-4bf6-8d16-8357c5e71181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into sentence embeddings\n",
    "t5_test['embeddings'] = t5_test['processed_text'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e79b8ff-7b19-44cc-8387-7e194e1afde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_embeddings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640547ea-e188-4522-bae2-a169accc8621",
   "metadata": {},
   "source": [
    "## Emotion Embedding with SBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ed1fc-e241-49fe-b649-69e955e9d3b4",
   "metadata": {},
   "source": [
    "This code reads the basic NRC Emotion Lexicon, filters out 'positive' and 'negative' sentiments, and creates emotion embeddings for each emotion. For each emotion, it groups the related words, and generates embeddings using the SBERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb54f954-0ae5-4419-8853-7ed0a5a2a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and read NRC Emotion file\n",
    "nrc_data_path = 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'  \n",
    "nrc_df = pd.read_csv(nrc_data_path, sep='\\t', header=None, names=['word', 'emotion', 'sentiment'])\n",
    "\n",
    "# Filters out 'positive' and 'negative' sentiments (don't count them)\n",
    "nrc_df = nrc_df[(nrc_df['sentiment'] == 1) & (~nrc_df['emotion'].isin(['positive', 'negative']))]\n",
    "\n",
    "# Get emotion from the word\n",
    "def get_emotion(word):\n",
    "    emotions = nrc_df[nrc_df['word'] == word]['emotion'].tolist()\n",
    "    if emotions:\n",
    "        return emotions\n",
    "    else:\n",
    "        return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef82704b-abee-4b43-b3e5-44c0c6a46708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence embeddings for each emotion in NRC Emotion Lexicon\n",
    "emotion_embeddings = {}\n",
    "nrc_emotions = nrc_df['emotion'].unique()\n",
    "\n",
    "# Create embeddings for each emotion by averaging the vectors of the words in the emotion group\n",
    "for emotion in nrc_emotions:\n",
    "    # Retrieve the words representing each emotion in the NRC\n",
    "    words = nrc_df[nrc_df['emotion'] == emotion]['word'].tolist()\n",
    "\n",
    "    # Generate embeddings for the emotion by concatenating the words in the emotion group\n",
    "    words_embeddings = model.encode(' '.join(words))  # create embeddings for each representative word\n",
    "    emotion_embeddings[emotion] = words_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3db73e-ad31-4d57-a915-cbdab0a39c31",
   "metadata": {},
   "source": [
    "## Predict emotion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eefef-2ebf-4376-a4bd-a67aa985dfdd",
   "metadata": {},
   "source": [
    "This code compares the similarity between text posts and emotions using cosine similarity. The get_most_similar_emotion function calculates the cosine similarity between the embedding of each text and the emotion embeddings. It selects the emotion with the highest similarity score as the predicted emotion for each text. The predicted emotion is then assigned to the predicted_emotion column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c29060c-3818-48a8-9b2b-767a3ed31d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between texts và emotions\n",
    "def get_most_similar_emotion(post_embedding):\n",
    "    max_sim = -1\n",
    "    most_similar_emotion = None\n",
    "    for emotion, emotion_embedding in emotion_embeddings.items():\n",
    "        similarity = cosine_similarity([post_embedding], [emotion_embedding])[0][0]\n",
    "        if similarity > max_sim:\n",
    "            max_sim = similarity\n",
    "            most_similar_emotion = emotion\n",
    "    return most_similar_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009fcda4-b473-4a12-a038-bd59514da5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict emotion by embedding\n",
    "t5_test['predicted_emotion'] = t5_test['embeddings'].apply(lambda x: get_most_similar_emotion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0ecd6c5-ba48-48af-b277-34c6217cfd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_emotion\n",
       "trust           10016\n",
       "anticipation     8591\n",
       "surprise         3188\n",
       "anger            1465\n",
       "sadness           608\n",
       "fear              371\n",
       "joy               208\n",
       "disgust            27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = t5_test['predicted_emotion'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f1a8d-cd26-4988-8c10-13b2ed576f0b",
   "metadata": {},
   "source": [
    "## Convert to Parrott's emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822406-efe3-4211-8221-c9db6cdcf5af",
   "metadata": {},
   "source": [
    "After predicting the NRC labels, we mapped our results to Parrott's emotion categories based on the definitions provided in the groups listed at  https://en.wikipedia.org/wiki/Emotion_classification. This mapping was an attempt to align the NRC emotions with Parrott's emotions to observe their correspondence.\n",
    "In this approach, we highlighted some changes as below:\n",
    "1. In class \"joy\" in Parrott's emotion, there are two sub-emotions: \"eagerness\" and \"hope.\" These emotions share similar meanings with \"anticipation\" from the NRC lexicon, so we decided to convert it into \"joy.\"\n",
    "2. In class \"anger\" in Parrott's emotion, there is the \"disgust\" in this list, so we decided to convert it into \"anger\"\n",
    "3. In class \"love\" in Parrott's emotion, we added \"trust\" because it’s an important part of love. Trust helps build strong, loving relationships, so we included it under \"love\" to show how essential it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e446f84-89da-49ea-a05a-abc22d60a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nrc_to_parrott(nrc_emotion):\n",
    "    mapping = {\n",
    "        \"anger\": \"anger\",\n",
    "        \"anticipation\": \"joy\",  # convert Anticipation to Joy\n",
    "        \"disgust\": \"anger\",   # convert Disgust to Anger\n",
    "        \"fear\": \"fear\",\n",
    "        \"joy\": \"joy\",\n",
    "        \"sadness\": \"sadness\",\n",
    "        \"surprise\": \"surprise\",\n",
    "        \"trust\": \"love\",         # convert Trust to Love\n",
    "    }\n",
    "    return mapping.get(nrc_emotion, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7d1138f-8d2c-4efe-96ed-a049c5c5363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the NRC's emotion label to the Parrott's emotion label\n",
    "t5_test['map_to_parrott'] = t5_test['predicted_emotion'].apply(map_nrc_to_parrott)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1725bf2-3a70-4dd3-ad50-33147de84fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map_to_parrott\n",
       "love        10016\n",
       "joy          8799\n",
       "surprise     3188\n",
       "anger        1492\n",
       "sadness       608\n",
       "fear          371\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of label after predicting\n",
    "label_counts_t5 = t5_test['map_to_parrott'].value_counts()\n",
    "label_counts_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23cae34d-9210-49b0-a26f-5c06093d0aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>predicted_emotion</th>\n",
       "      <th>map_to_parrott</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Winter Blues and WFH question i think the sudd...</td>\n",
       "      <td>anger</td>\n",
       "      <td>winter blue wfh question think sudden shift bu...</td>\n",
       "      <td>[0.019767283, -0.023702554, -0.024428228, 0.03...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Workspace i saw some of your other posts a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>new workspace see post renovation go incredibl...</td>\n",
       "      <td>[-0.0148920305, 0.053874586, 0.008216122, 0.00...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hard to mentally unwind… i go for a long walk ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>hard mentally go long walk work help shift wor...</td>\n",
       "      <td>[-0.005250863, -0.021374501, -0.04698471, -0.0...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Would you leave 150k for 90k depends on your e...</td>\n",
       "      <td>joy</td>\n",
       "      <td>would leave depend expense saving</td>\n",
       "      <td>[0.0018950137, 0.006368839, -0.0058382694, -0....</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There’s no magic formula to get remote work so...</td>\n",
       "      <td>fear</td>\n",
       "      <td>magic formula get remote work worry nothing go...</td>\n",
       "      <td>[-0.01843844, -0.03417755, -0.0084715495, -0.0...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Winter Blues and WFH question i think the sudd...  anger   \n",
       "1  New Workspace i saw some of your other posts a...    joy   \n",
       "2  Hard to mentally unwind… i go for a long walk ...    joy   \n",
       "3  Would you leave 150k for 90k depends on your e...    joy   \n",
       "4  There’s no magic formula to get remote work so...   fear   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  winter blue wfh question think sudden shift bu...   \n",
       "1  new workspace see post renovation go incredibl...   \n",
       "2  hard mentally go long walk work help shift wor...   \n",
       "3                  would leave depend expense saving   \n",
       "4  magic formula get remote work worry nothing go...   \n",
       "\n",
       "                                          embeddings predicted_emotion  \\\n",
       "0  [0.019767283, -0.023702554, -0.024428228, 0.03...      anticipation   \n",
       "1  [-0.0148920305, 0.053874586, 0.008216122, 0.00...               joy   \n",
       "2  [-0.005250863, -0.021374501, -0.04698471, -0.0...           sadness   \n",
       "3  [0.0018950137, 0.006368839, -0.0058382694, -0....           sadness   \n",
       "4  [-0.01843844, -0.03417755, -0.0084715495, -0.0...          surprise   \n",
       "\n",
       "  map_to_parrott  \n",
       "0            joy  \n",
       "1            joy  \n",
       "2        sadness  \n",
       "3        sadness  \n",
       "4       surprise  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3d7ce-c4d7-4915-8665-1f290584aa38",
   "metadata": {},
   "source": [
    "## Yangswei_85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14c2bc-57e2-4296-9525-ca0e39c92c06",
   "metadata": {},
   "source": [
    "From this part, we implemented the code to the yangswei_85 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0de6f96-a43d-44d7-b014-e5400ae69fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "yangswei_85_test = pd.read_csv('https://raw.githubusercontent.com/SaraHoxha/emotion-detection-txa/main/Model%20Implementation/data/test_yangswei_85.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3aa7b6a-0989-4047-871f-790261de8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "yangswei_85_test['processed_text'] = yangswei_85_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0077088c-ab27-4567-8b4f-9db880831eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into sentence embeddings\n",
    "yangswei_85_test['embeddings'] = yangswei_85_test['processed_text'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6dfdbaf-1488-4233-a5aa-4d2936c803e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict emotion by embedding\n",
    "yangswei_85_test['predicted_emotion'] = yangswei_85_test['embeddings'].apply(lambda x: get_most_similar_emotion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b88a96c-6a8b-476f-9bc7-fc05d664ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the NRC's emotion label to the Parrott's emotion label\n",
    "yangswei_85_test['map_to_parrott'] = yangswei_85_test['predicted_emotion'].apply(map_nrc_to_parrott)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a8ab02-14f2-4ed1-9b8f-d773c5de69d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>predicted_emotion</th>\n",
       "      <th>map_to_parrott</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTO is the new war on the middle class don't f...</td>\n",
       "      <td>joy</td>\n",
       "      <td>rto new war middle class fire tie package aver...</td>\n",
       "      <td>[-0.023674954, -0.005235405, 0.007556416, -0.0...</td>\n",
       "      <td>trust</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you continue with life outside of work ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>continue life outside work mental exhaustion c...</td>\n",
       "      <td>[0.0020819395, 0.042672526, -0.032852497, -0.0...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very desperate for a job would you know a pers...</td>\n",
       "      <td>fear</td>\n",
       "      <td>desperate job would know person come apply job...</td>\n",
       "      <td>[0.02066167, 0.011141692, -0.021646924, -0.056...</td>\n",
       "      <td>trust</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What time do you start working most days quest...</td>\n",
       "      <td>joy</td>\n",
       "      <td>time start work day question set hour like get...</td>\n",
       "      <td>[-0.04142596, -0.0028264832, -0.00038986365, -...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are good job sites to find LEGIT remote w...</td>\n",
       "      <td>joy</td>\n",
       "      <td>good job site find legit remote work like dece...</td>\n",
       "      <td>[-0.027872037, 0.0032044589, -0.03555838, -0.0...</td>\n",
       "      <td>trust</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  RTO is the new war on the middle class don't f...   joy   \n",
       "1  How do you continue with life outside of work ...   joy   \n",
       "2  Very desperate for a job would you know a pers...  fear   \n",
       "3  What time do you start working most days quest...   joy   \n",
       "4  What are good job sites to find LEGIT remote w...   joy   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  rto new war middle class fire tie package aver...   \n",
       "1  continue life outside work mental exhaustion c...   \n",
       "2  desperate job would know person come apply job...   \n",
       "3  time start work day question set hour like get...   \n",
       "4  good job site find legit remote work like dece...   \n",
       "\n",
       "                                          embeddings predicted_emotion  \\\n",
       "0  [-0.023674954, -0.005235405, 0.007556416, -0.0...             trust   \n",
       "1  [0.0020819395, 0.042672526, -0.032852497, -0.0...      anticipation   \n",
       "2  [0.02066167, 0.011141692, -0.021646924, -0.056...             trust   \n",
       "3  [-0.04142596, -0.0028264832, -0.00038986365, -...      anticipation   \n",
       "4  [-0.027872037, 0.0032044589, -0.03555838, -0.0...             trust   \n",
       "\n",
       "  map_to_parrott  \n",
       "0           love  \n",
       "1            joy  \n",
       "2           love  \n",
       "3            joy  \n",
       "4           love  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yangswei_85_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ec24780-6ec5-47c5-8b93-bcbda6c5ab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map_to_parrott\n",
       "love        9092\n",
       "joy         7979\n",
       "surprise    2970\n",
       "anger       1337\n",
       "sadness      575\n",
       "fear         336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of label after predicting\n",
    "label_counts_yangswei_85 = yangswei_85_test['map_to_parrott'].value_counts()\n",
    "label_counts_yangswei_85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66d8b8-02ac-4184-9393-9dc5e8066e2e",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7240df59-e491-482e-a9ab-e9cbdc6f2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall,\n",
    "        'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8d09e1a-581d-478a-87dd-a1082d5715f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.23719048786467273,\n",
       " 'precision': 0.481811500158746,\n",
       " 'recall': 0.23719048786467273,\n",
       " 'f1': 0.2871546505038804}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of t5 dataset\n",
    "y_true_t5 = t5_test['label']\n",
    "y_pred_t5 = t5_test['map_to_parrott']\n",
    "t5_metrics_map_to_parrott = calculate_metrics(y_true_t5, y_pred_t5)\n",
    "t5_metrics_map_to_parrott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b71aec19-38d1-46e4-89b4-3ecb68fdaaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.2595002018933106,\n",
       " 'precision': 0.4941631108057709,\n",
       " 'recall': 0.2595002018933106,\n",
       " 'f1': 0.3281493426504335}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of yangswei_85 dataset\n",
    "y_true_yangswei_85_test = yangswei_85_test['label']\n",
    "y_pred_yangswei_85_test = yangswei_85_test['map_to_parrott']\n",
    "yangswei_85_metrics_map_to_parrott = calculate_metrics(y_true_yangswei_85_test, y_pred_yangswei_85_test)\n",
    "yangswei_85_metrics_map_to_parrott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "063ca108-8cfa-47b1-969f-1526453192fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metrics to the result file\n",
    "def save_metrics_to_file(metrics, filename):\n",
    "    metrics_str = (f\"Accuracy: {metrics['accuracy']:.4f}\\n\"\n",
    "        f\"Precision: {metrics['precision']:.4f}\\n\"\n",
    "        f\"Recall: {metrics['recall']:.4f}\\n\"\n",
    "        f\"F1-Score: {metrics['f1']:.4f}\\n\")\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(metrics_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90182f19-1f82-4620-8b48-0eaa10544cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_to_file(t5_metrics_map_to_parrott, 't5_metrics_nrc.txt')\n",
    "save_metrics_to_file(yangswei_85_metrics_map_to_parrott, 'yangswei_85_metrics_nrc.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
