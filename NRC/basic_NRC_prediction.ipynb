{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044a951d-2069-4a6f-8044-6c7efdfdd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf898f-ab5c-459e-92eb-d59a1080b500",
   "metadata": {},
   "source": [
    "## Predict emotion by NRC lexicon (dont' use the sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c703f7f-4ec9-423b-9d9a-b8ef4771b634",
   "metadata": {},
   "source": [
    "### T5 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5054f17-ea45-44e2-a450-2556686f9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_test = pd.read_csv('test_t5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09adbee1-8e15-4eda-b3ac-885e993f7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion_nrclex_ignore_sentiments(text):\n",
    "    emotion_data = NRCLex(text)\n",
    "    # don't count \"positive\" and \"negative\" sentiment\n",
    "    filtered_emotions = {emotion: score for emotion, score in emotion_data.raw_emotion_scores.items()\n",
    "                         if emotion not in ['positive', 'negative']}\n",
    "    \n",
    "    if filtered_emotions:\n",
    "        # choose the label having the highest score\n",
    "        dominant_emotion = max(filtered_emotions, key=filtered_emotions.get)\n",
    "        return dominant_emotion\n",
    "    else:\n",
    "        return \"neutral\"  # return \"neutral if the model could not detect any emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4389f5d-f1de-4472-934b-ae34a00d3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_test['label_nrc'] = t5_test['text'].apply(predict_emotion_nrclex_ignore_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b724f6e-b7f4-48b2-94b4-bad92922e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_test.to_csv('t5_test_nrc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dca4c-5d82-4ef7-aff0-1834c3a44f0d",
   "metadata": {},
   "source": [
    "### Yangswei_85 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee931e8e-11dd-4c76-9b18-e1d01606ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yangswei_85_test = pd.read_csv('test_yangswei_85.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e15ba6-317f-4a7f-9b47-768bdd013cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "yangswei_85_test['label_nrc'] = yangswei_85_test['text'].apply(predict_emotion_nrclex_ignore_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c60438a7-ab9f-43cc-be23-07b389167b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_nrc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTO is the new war on the middle class don't f...</td>\n",
       "      <td>joy</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you continue with life outside of work ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very desperate for a job would you know a pers...</td>\n",
       "      <td>fear</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What time do you start working most days quest...</td>\n",
       "      <td>joy</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are good job sites to find LEGIT remote w...</td>\n",
       "      <td>joy</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label     label_nrc\n",
       "0  RTO is the new war on the middle class don't f...   joy          fear\n",
       "1  How do you continue with life outside of work ...   joy  anticipation\n",
       "2  Very desperate for a job would you know a pers...  fear  anticipation\n",
       "3  What time do you start working most days quest...   joy  anticipation\n",
       "4  What are good job sites to find LEGIT remote w...   joy         trust"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yangswei_85_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a289cbd2-b6b8-47a0-aa62-200646b29829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_nrc\n",
       "anticipation    5776\n",
       "trust           5199\n",
       "neutral         3363\n",
       "anger           2357\n",
       "joy             1689\n",
       "sadness         1510\n",
       "fear            1395\n",
       "disgust          521\n",
       "surprise         479\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = yangswei_85_test['label_nrc'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aad05767-7b42-433d-8da2-f4a54d7de4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yangswei_85_test.to_csv('yangswei_85_test_nrc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7c699-ce5f-4068-9963-0d4fad7ad51c",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a82b70-a254-4e4f-8f3f-8272091b2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall,\n",
    "        'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e56ac1a-ba08-4acc-84aa-aa53ffa06115",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_t5 = t5_test['label']\n",
    "y_pred_t5 = t5_test['label_nrc']\n",
    "y_true_yangswei_85 = yangswei_85_test['label']\n",
    "y_pred_yangswei_85 = yangswei_85_test['label_nrc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30774d3b-c656-4444-b23e-e2f13835993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.12351883631609055,\n",
       " 'precision': 0.47043178625411086,\n",
       " 'recall': 0.12351883631609055,\n",
       " 'f1': 0.1883580531231452}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_metrics = calculate_metrics(y_true_t5, y_pred_t5)\n",
    "t5_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96920fdb-c55e-40f1-ba7c-73ebba743673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\minhd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.1096056350666248,\n",
       " 'precision': 0.524316457758694,\n",
       " 'recall': 0.1096056350666248,\n",
       " 'f1': 0.16550956179207252}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yangswei_85_metrics = calculate_metrics(y_true_yangswei_85, y_pred_yangswei_85)\n",
    "yangswei_85_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b857992d-ccb4-47a4-9e26-31123d3f6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_file(metrics, filename):\n",
    "    metrics_str = (f\"Accuracy: {metrics['accuracy']:.4f}\\n\"\n",
    "        f\"Precision: {metrics['precision']:.4f}\\n\"\n",
    "        f\"Recall: {metrics['recall']:.4f}\\n\"\n",
    "        f\"F1-Score: {metrics['f1']:.4f}\\n\")\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(metrics_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba4cfdbc-50fa-417c-83aa-0f8576920aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_to_file(t5_metrics, 't5_metrics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08ac2420-7770-4d2f-ae8f-9b570e75fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_to_file(yangswei_85_metrics, 'yangswei_85_metrics.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
