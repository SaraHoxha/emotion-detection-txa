{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib                  # 2D Plotting Library\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  \n",
    "from utils import preprocess_text\n",
    "\n",
    "#Paste the test data to test on the model\n",
    "file =\"train.csv\"\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA HANDLING BEFORE XAI ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading x_test and y_test\n",
    "X_test = df['processed_text']\n",
    "y_test = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=5)\n",
    "vect = CountVectorizer(min_df=5)  # Convert a collection of text documents to a matrix of token counts.\n",
    "X_test_tok = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD THE SVM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "clf = ''\n",
    "predi = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, predi)}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_pipeline(vectorizer, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making explainable predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Text:'+df[\"text\"][0])\n",
    "class_names = c.classes_\n",
    "probabilities = c.predict_proba([df[\"text\"][0]])[0]\n",
    "label_probabilities = dict(zip(class_names, probabilities))\n",
    "print('predict_proba:'+ str(label_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "X_test = X_test.tolist()\n",
    "y_test = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK WHAT LABEL YOU WANT FOR THE XAI EXPLANAITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing ids of comments which are toxic\n",
    "\"\"\" count=-1\n",
    "for x in y_test:\n",
    "    count=count+1\n",
    "    if x==1:\n",
    "        print(count) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR LABEL N (of all the emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 26\n",
    "exp_nontoxic = explainer.explain_instance(X_test[idx], c.predict_proba, num_features=10)\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(toxic) =', c.predict_proba([X_test[idx]])[0,1])\n",
    "print('True class: %s' % class_names[y_test[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nontoxic.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original prediction:', clf.predict_proba(X_test_tok[idx])[0,1])\n",
    "tmp = X_test_tok[idx].copy()\n",
    "########CHANGE THE WORDS THAT YOU WANT TO ANALIZE FROM TEXT\n",
    "tmp[0,vectorizer.vocabulary_['talk']] = 0\n",
    "tmp[0,vectorizer.vocabulary_['pretending']] = 0\n",
    "print('Prediction removing some features:', clf.predict_proba(tmp)[0,1])\n",
    "print('Difference:', clf.predict_proba(tmp)[0,1] - clf.predict_proba(X_test_tok[idx])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = exp_nontoxic.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nontoxic.show_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
